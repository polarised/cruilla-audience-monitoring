{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5711af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TW2025-04-28_19.54h (Cruilla 2024).json: 1245 → 486 no duplicates\n",
      "TW2025-04-28_20.12h (Cruilla 2024).json: 1245 → 486 no duplicates\n",
      "TW2025-04-28_20.18h (Cruilla 2024).json: 1245 → 486 no duplicates\n",
      "TW2025-04-28_20.20h (Cruilla 2024).json: 1063 → 415 no duplicates\n",
      "TW2025-04-28_20.28h (Cruilla 2024).json: 441 → 185 no duplicates\n",
      "TW2025-04-28_20.45h (Cruilla 2024).json: 262 → 108 no duplicates\n",
      "TW2025-04-28_22.17h (Cruilla 2023).json: 710 → 316 no duplicates\n",
      "TW2025-04-28_22.18h (Cruilla 2023).json: 538 → 288 no duplicates\n",
      "TW2025-04-28_22.19h (Cruilla 2023).json: 651 → 288 no duplicates\n",
      "TW2025-04-28_22.20h (Cruilla 2023).json: 491 → 227 no duplicates\n",
      "TW2025-04-28_22.21h (Cruilla 2022).json: 506 → 217 no duplicates\n",
      "TW2025-04-28_22.21h (Cruilla 2023).json: 491 → 227 no duplicates\n",
      "TW2025-04-28_22.22h (Cruilla 2022).json: 506 → 217 no duplicates\n",
      "TW2025-04-28_22.23h (Cruilla 2022).json: 410 → 175 no duplicates\n",
      "TW2025-04-28_22.24h (Cruilla 2022).json: 410 → 175 no duplicates\n",
      "TW2025-04-28_22.25h (Cruilla 2022).json: 292 → 121 no duplicates\n",
      "TW2025-04-28_22.26h (Cruilla 2022).json: 292 → 121 no duplicates\n",
      "TW2025-04-28_22.32h (Cruilla 2021).json: 745 → 324 no duplicates\n",
      "TW2025-04-28_22.34h (Cruilla 2021).json: 745 → 324 no duplicates\n",
      "TW2025-04-28_22.35h (Cruilla 2021).json: 619 → 273 no duplicates\n",
      "TW2025-04-28_22.36h (Cruilla 2021).json: 619 → 273 no duplicates\n",
      "TW2025-04-28_22.37h (Cruilla 2021).json: 424 → 192 no duplicates\n",
      "TW2025-04-28_22.38h (Cruilla 2021).json: 424 → 192 no duplicates\n",
      "TW2025-04-28_22.41h (Cruilla 2020).json: 612 → 265 no duplicates\n",
      "TW2025-04-28_22.42h (Cruilla 2020).json: 650 → 265 no duplicates\n",
      "TW2025-04-29_22.14h (Cruilla 2023).json: 710 → 316 no duplicates\n",
      "TW2025-04-29_22.49h (Cruilla 2019).json: 1047 → 470 no duplicates\n",
      "TW2025-04-29_22.51h (Cruilla 2019).json: 1047 → 470 no duplicates\n",
      "TW2025-04-29_22.53h (Cruilla 2019).json: 870 → 388 no duplicates\n",
      "TW2025-04-29_22.55h (Cruilla 2019).json: 870 → 388 no duplicates\n",
      "TW2025-04-29_22.57h (Cruilla 2019).json: 870 → 388 no duplicates\n",
      "TW2025-04-29_23.02h (Cruilla 2019).json: 617 → 280 no duplicates\n",
      "TW2025-04-29_23.06h (Cruilla 2019).json: 617 → 280 no duplicates\n",
      "TW2025-04-29_23.07h (Cruilla 2018).json: 753 → 304 no duplicates\n",
      "TW2025-04-29_23.08h (Cruilla 2018).json: 753 → 304 no duplicates\n",
      "TW2025-04-29_23.09h (Cruilla 2018).json: 623 → 255 no duplicates\n",
      "TW2025-04-29_23.10h (Cruilla 2018).json: 623 → 255 no duplicates\n",
      "TW2025-04-29_23.11h (Cruilla 2018).json: 467 → 187 no duplicates\n",
      "TW2025-04-29_23.12h (Cruilla 2018).json: 467 → 187 no duplicates\n",
      "TW2025-04-29_23.13h (Cruilla 2017).json: 1160 → 523 no duplicates\n",
      "TW2025-04-29_23.15h (Cruilla 2017).json: 1160 → 523 no duplicates\n",
      "TW2025-04-29_23.16h (Cruilla 2017).json: 1011 → 452 no duplicates\n",
      "TW2025-04-29_23.18h (Cruilla 2017).json: 987 → 452 no duplicates\n",
      "TW2025-04-29_23.19h (Cruilla 2017).json: 784 → 343 no duplicates\n",
      "TW2025-04-29_23.20h (Cruilla 2017).json: 784 → 343 no duplicates\n",
      "TW2025-04-29_23.22h (Cruilla 2016).json: 1249 → 544 no duplicates\n",
      "TW2025-04-29_23.23h (Cruilla 2016).json: 1249 → 544 no duplicates\n",
      "TW2025-04-29_23.24h (Cruilla 2016).json: 1013 → 445 no duplicates\n",
      "TW2025-04-29_23.25h (Cruilla 2016).json: 1013 → 445 no duplicates\n",
      "TW2025-04-29_23.27h (Cruilla 2016).json: 1013 → 445 no duplicates\n",
      "TW2025-04-29_23.28h (Cruilla 2016).json: 733 → 321 no duplicates\n",
      "TW2025-04-29_23.30h (Cruilla 2016).json: 733 → 321 no duplicates\n",
      "TW2025-04-29_23.31h (Cruilla 2015).json: 758 → 306 no duplicates\n",
      "TW2025-04-29_23.32h (Cruilla 2015).json: 758 → 306 no duplicates\n",
      "TW2025-04-29_23.33h (Cruilla 2015).json: 678 → 271 no duplicates\n",
      "TW2025-04-29_23.34h (Cruilla 2015).json: 678 → 271 no duplicates\n",
      "TW2025-04-29_23.35h (Cruilla 2015).json: 518 → 210 no duplicates\n",
      "TW2025-04-29_23.36h (Cruilla 2015).json: 518 → 210 no duplicates\n",
      "TW2025-04-29_23.38h (Cruilla 2014).json: 718 → 288 no duplicates\n",
      "TW2025-04-29_23.39h (Cruilla 2014).json: 718 → 288 no duplicates\n",
      "TW2025-04-29_23.40h (Cruilla 2014).json: 316 → 203 no duplicates\n",
      "TW2025-04-29_23.41h (Cruilla 2014).json: 499 → 203 no duplicates\n",
      "TW2025-04-29_23.42h (Cruilla 2014).json: 311 → 129 no duplicates\n",
      "TW2025-04-29_23.43h (Cruilla 2014).json: 311 → 129 no duplicates\n",
      "TW2025-04-29_23.44h (Cruilla 2014).json: 220 → 129 no duplicates\n",
      "TW2025-05-07_13.39h (Cruilla 2022).json: 4721 → 4459 no duplicates\n",
      "TW2025-05-07_13.39h (Cruilla 2024).json: 3930 → 3759 no duplicates\n",
      "TW2025-05-07_14.29h (Cruilla 2024).json: 22012 → 18071 no duplicates\n",
      "TW2025-05-07_14.39h (Cruilla 2024).json: 4034 → 3860 no duplicates\n",
      "TW2025-05-07_14.53h (Cruilla 2024).json: 7814 → 4045 no duplicates\n",
      "TW2025-05-07_15.39h (Cruilla 2023).json: 39814 → 18071 no duplicates\n",
      "TW2025-05-07_16.39h (Cruilla 2021).json: 4349 → 4057 no duplicates\n",
      "TW2025-05-07_16.53h (Cruilla 2019).json: 21929 → 17987 no duplicates\n",
      "TW2025-05-07_17.01h (Cruilla 2018).json: 1196 → 1194 no duplicates\n",
      "TW2025-05-07_17.06h (Cruilla 2017).json: 1566 → 1565 no duplicates\n",
      "TW2025-05-07_17.13h (Cruilla 2016).json: 1994 → 1994 no duplicates\n",
      "TW2025-05-07_17.17h (Cruilla 2015).json: 1680 → 1680 no duplicates\n",
      "TW2025-05-07_17.21h (Cruilla 2014).json: 1065 → 1065 no duplicates\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "carpeta_origen = r\"C:\\Users\\migue\\OneDrive\\Escritorio\\UAB_INTELIGENCIA_ARTIFICIAL\\Tercer_Any\\3B\\Synthesis Project II\\Limpieza\\TW (Cruilla 2014-2024)\"    \n",
    "carpeta_destino = r\"C:\\Users\\migue\\OneDrive\\Escritorio\\UAB_INTELIGENCIA_ARTIFICIAL\\Tercer_Any\\3B\\Synthesis Project II\\Limpieza\\CL - TW (Cruilla 2014-2024)\"  \n",
    "\n",
    "os.makedirs(carpeta_destino, exist_ok=True)\n",
    "\n",
    "TWEET = [\n",
    "    \"id\", \"url\", \"fullText\", \"retweetCount\",\"replyCount\",\"likeCount\",\n",
    "    \"quoteCount\", \"viewCount\", \"createdAt\", \"lang\", \"isReply\", \"conversationId\",\n",
    "    \"isRetweet\"\n",
    "]\n",
    "AUTHOR = [\n",
    "    \"userName\", \"followers\", \"following\", \"location\"\n",
    "]\n",
    "\n",
    "for archivo in os.listdir(carpeta_origen):\n",
    "    if archivo.endswith(\".json\"):\n",
    "        ruta_archivo = os.path.join(carpeta_origen, archivo)\n",
    "\n",
    "        with open(ruta_archivo, \"r\", encoding=\"utf-8\") as f:\n",
    "            datos = json.load(f)\n",
    "\n",
    "        tweets_limpios = []\n",
    "        ids_vistos = set()\n",
    "\n",
    "        for tweet in datos:\n",
    "            tweet_id = tweet.get(\"id\")\n",
    "            if tweet_id in ids_vistos:\n",
    "                continue  \n",
    "            ids_vistos.add(tweet_id)\n",
    "\n",
    "            limpio = {campo: tweet.get(campo) for campo in TWEET}\n",
    "            if \"author\" in tweet:\n",
    "                limpio[\"author\"] = {campo: tweet[\"author\"].get(campo) for campo in AUTHOR}\n",
    "            tweets_limpios.append(limpio)\n",
    "\n",
    "        ruta_guardado = os.path.join(carpeta_destino, archivo)\n",
    "        with open(ruta_guardado, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(tweets_limpios, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"{archivo}: {len(datos)} → {len(tweets_limpios)} no duplicates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c26fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV creado con 101569\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "carpeta_json_limpios = r\"C:\\Users\\migue\\OneDrive\\Escritorio\\UAB_INTELIGENCIA_ARTIFICIAL\\Tercer_Any\\3B\\Synthesis Project II\\Limpieza\\CL - TW (Cruilla 2014-2024)\"\n",
    "csv_tweets = r\"C:\\Users\\migue\\OneDrive\\Escritorio\\UAB_INTELIGENCIA_ARTIFICIAL\\Tercer_Any\\3B\\Synthesis Project II\\Limpieza\\all_tweets.csv\"\n",
    "\n",
    "filas = []\n",
    "\n",
    "for archivo in os.listdir(carpeta_json_limpios):\n",
    "    if archivo.endswith(\".json\"):\n",
    "        ruta_archivo = os.path.join(carpeta_json_limpios, archivo)\n",
    "        with open(ruta_archivo, \"r\", encoding=\"utf-8\") as f:\n",
    "            datos = json.load(f)\n",
    "\n",
    "        for tweet in datos:\n",
    "            fila = {campo: tweet.get(campo, \"\") for campo in TWEET}\n",
    "            autor = tweet.get(\"author\", {})\n",
    "            for campo_autor in AUTHOR:\n",
    "                fila[f\"author_{campo_autor}\"] = autor.get(campo_autor, \"\")\n",
    "            filas.append(fila)\n",
    "\n",
    "campos_csv = TWEET + [f\"author_{campo}\" for campo in AUTHOR]\n",
    "\n",
    "with open(csv_tweets, \"w\", encoding=\"utf-8\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=campos_csv)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(filas)\n",
    "\n",
    "print(f\"CSV creado con {len(filas)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52c15bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101569 entries, 0 to 101568\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   id                101569 non-null  int64  \n",
      " 1   url               101569 non-null  object \n",
      " 2   fullText          101569 non-null  object \n",
      " 3   retweetCount      101569 non-null  int64  \n",
      " 4   replyCount        101569 non-null  int64  \n",
      " 5   likeCount         101569 non-null  int64  \n",
      " 6   quoteCount        101569 non-null  int64  \n",
      " 7   viewCount         15958 non-null   float64\n",
      " 8   createdAt         101569 non-null  object \n",
      " 9   lang              101569 non-null  object \n",
      " 10  isReply           101569 non-null  bool   \n",
      " 11  conversationId    101569 non-null  int64  \n",
      " 12  isRetweet         101569 non-null  bool   \n",
      " 13  author_userName   101569 non-null  object \n",
      " 14  author_followers  101569 non-null  int64  \n",
      " 15  author_following  101569 non-null  int64  \n",
      " 16  author_location   87362 non-null   object \n",
      "dtypes: bool(2), float64(1), int64(8), object(6)\n",
      "memory usage: 11.8+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(csv_tweets, encoding=\"utf-8\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34980a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalizar_fulltext(texto):\n",
    "    if pd.isna(texto):\n",
    "        return texto\n",
    "    return texto.lower()\n",
    "\n",
    "df[\"fullText\"] = df[\"fullText\"].apply(normalizar_fulltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c75720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_29552\\1771910675.py:33: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  filtro_reventa = df[\"fullText\"].str.contains(patron_reventa, case=False, na=False, regex=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV limpio guardado con éxito.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "\n",
    "# ========= 1. ELIMINAR TWEETS DE REVENTA =========\n",
    "\n",
    "patrones_reventa = [\n",
    "    # Español\n",
    "    r\"\\b(vendo|vendemos|vendiendo|se venden|vendo\\s+\\d+|vendo\\s+una|vendo\\s+varias)\\s+(entradas?|abonos?)\\b\",\n",
    "    r\"\\b(compro|compramos|comprando|busco|necesito)\\s+(entradas?|abonos?)\\b\",\n",
    "    r\"\\b(cambio|cambiamos|cambiando)\\s+(entradas?|abonos?)\\b\",\n",
    "    r\"\\b(tengo|ofrezco|dispongo de)\\s+(entradas?|abonos?)\\b\",\n",
    "    r\"\\b(sorteo|sorteamos|sorteando)\\s+(entradas?|abonos?)\\b\",\n",
    "    r\"\\b(gana|ganar|consigue|consigues)\\s+(entradas?|abonos?)\\b\",\n",
    "    r\"\\b(venc|vendo|busco|vendemos|venden)\\b\",\n",
    "\n",
    "    # Catalán\n",
    "    r\"\\b(venem|venc|venent|es venen)\\s+(entrades?|abonaments?)\\b\",\n",
    "    r\"\\b(compro|comprem|busco|necessito|necessitem)\\s+(entrades?|abonaments?)\\b\",\n",
    "    r\"\\b(canvio|canviem|canviant)\\s+(entrades?|abonaments?)\\b\",\n",
    "    r\"\\b(tinc|ofereixo|disposo de)\\s+(entrades?|abonaments?)\\b\",\n",
    "    r\"\\b(sorteig|sorteigem|sortegem|sortegen)\\s+(entrades?|abonaments?)\\b\",\n",
    "    r\"\\b(guanya|guanyar)\\s+(entrades?|abonaments?)\\b\",\n",
    "\n",
    "    # Inglés\n",
    "    r\"\\b(sell|selling|resell|reselling|buy|buying)\\s+(tickets?|passes|entries?)\\b\",\n",
    "    r\"\\btickets?\\s+(for sale|available|to sell|giveaway|give away)\\b\",\n",
    "    r\"\\b(win|winning|get|claim)\\s+(tickets?|passes|entries?)\\b\",\n",
    "]\n",
    "\n",
    "patron_reventa = '|'.join(patrones_reventa)\n",
    "filtro_reventa = df[\"fullText\"].str.contains(patron_reventa, case=False, na=False, regex=True)\n",
    "df = df[~filtro_reventa]\n",
    "\n",
    "# ========= 2. QUITAR DUPLICADOS POR ID =========\n",
    "\n",
    "df = df.drop_duplicates(subset=[\"id\"], keep=\"first\")\n",
    "\n",
    "# ========= 3. LIMPIAR TEXTOS: EMOJIS Y HASHTAGS =========\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    if pd.isna(texto):\n",
    "        return texto\n",
    "    texto = re.sub(r'http\\S+|www.\\S+', '', texto)\n",
    "    texto = re.sub(r'https\\S+|www.\\S+', '', texto)\n",
    "    # Quitar menciones @\n",
    "    texto = re.sub(r'@\\w+', '', texto)\n",
    "    # Quitar hashtags\n",
    "    texto = re.sub(r'#\\w+', '', texto)\n",
    "    texto = emoji.replace_emoji(texto, replace='')\n",
    "    # Quitar espacios extra\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    return texto\n",
    "\n",
    "df[\"fullText\"] = df[\"fullText\"].apply(limpiar_texto)\n",
    "\n",
    "# ========= GUARDAR NUEVO CSV LIMPIO =========\n",
    "\n",
    "df.to_csv(csv_tweets, index=False, encoding=\"utf-8\")\n",
    "print(\"CSV limpio guardado con éxito.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9dd76e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "csv_tweets = r\"C:\\Users\\migue\\OneDrive\\Escritorio\\UAB_INTELIGENCIA_ARTIFICIAL\\Tercer_Any\\3B\\Synthesis Project II\\Limpieza\\all_tweets.csv\"\n",
    "df = pd.read_csv(csv_tweets, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659b15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autores con más tweets:\n",
      "author_userName\n",
      "yulianavk          203\n",
      "amnistiacat         68\n",
      "lluislazaro         61\n",
      "canalte             55\n",
      "lenadorfilms        55\n",
      "elenanorabioso      43\n",
      "mmarsuarez          42\n",
      "nuriaar1969         41\n",
      "farandularty        37\n",
      "lufebrero           37\n",
      "vinnicia7365        35\n",
      "isidorajamett       34\n",
      "albertmaruny        33\n",
      "elisenda39          33\n",
      "bunburyuniverso     31\n",
      "oiacat              31\n",
      "revers__en          30\n",
      "jorge_morotxo       29\n",
      "cronicaglobal       28\n",
      "biancadevilar       28\n",
      "Name: count, dtype: int64\n",
      "Tweets eliminados: 13061\n",
      "Tweets: 11382\n",
      "['arasogranollers' 'archivocovid' 'daniel_spawn' ... 'helenagm' 'msolbun'\n",
      " 'glauco42487234']\n",
      "[1792  293  962 ... 2192 1275 2645]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_29552\\4155906267.py:45: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"createdAt\"] = pd.to_datetime(df[\"createdAt\"], errors=\"coerce\")\n"
     ]
    }
   ],
   "source": [
    "marcas = [\n",
    "    \"cruillabcn\", \"barcelonaglobal\", \"btvnoticies\", \"lavanguardia\", \"diaridebcn\", \n",
    "    \"cacaolat\", \"estrelladamm\", \"rodalies\", \"catalannews\", \"movistar_es\", \n",
    "    \"elpaiscatalunya\", \"radioudeg\", \"324cat\", \"naciodigital\", \"spotify\", \n",
    "    \"on_economia\", \"on_economia_cat\", \"diariregio7\", \"elpeoriodico\", \"msn_es\", \"Cati_Tarragona\", \"beteve\", \n",
    "    \"MBecerraTour\", \"nuvol_com\", \"Canal21Ebre\", \"MPC_management\", \"elnacionalcat_e\",\n",
    "    \"EPSI_UAB\", \"mundodeportivo\", \"cati_tarragona\", \"elnacionalcat\", \"los40\",\n",
    "    \"CatalunyaPlural\", \"ClubRACC\", \"rac105\", \"AlDiaCat\", \"bnmallorca\",\n",
    "    \"subnoise\", \"canal21ebre\", \"ebredigitalcat\", \"aguaitacat\", \"ElReferente\",\n",
    "    \"actualidad_na\", \"ElNacionalWeb\", \"EuropaFMCat\", \"MWCapital\", \"laganzuaweb\",\n",
    "    \"primerafilacat\", \n",
    "\n",
    "]\n",
    "\n",
    "df[\"author_userName\"] = df[\"author_userName\"].astype(str).str.lower()\n",
    "\n",
    "es_marca = df[\"author_userName\"].isin(marcas)\n",
    "\n",
    "keywords_cuentas = [\"turismo\",\"turisme\",\"info\",\"news\",\"diari\", \"diario \", \"noticias\", \"info\", \"informacion\", \"new\",\n",
    "                    \"radio\", \"barcelona\", \"tele\", \"es\", \"noticies\", \"tv\", \"magazine\", \"revista\", \"bcn\",\n",
    "                    \"editorial\", \"hotel\", \"biblioteca\", \"spain\", \"racc\", \"web\"]\n",
    "es_cuenta_info = df[\"author_userName\"].str.contains('|'.join(keywords_cuentas), na=False)\n",
    "\n",
    "df_marcas = df[es_marca | es_cuenta_info][[\"url\", \"author_userName\", \n",
    "                                           \"replyCount\"]].copy()\n",
    "\n",
    "df = df[~(es_marca | es_cuenta_info)]\n",
    "\n",
    "top_autores = df[\"author_userName\"].value_counts().head(20)  # top 20\n",
    "\n",
    "print(\"Autores con más tweets:\")\n",
    "print(top_autores)\n",
    "\n",
    "print(f\"Tweets eliminados: {len(df_marcas)}\")\n",
    "print(f\"Tweets: {len(df)}\")\n",
    "print(df[\"author_userName\"].unique())\n",
    "print(df[\"author_following\"].unique())\n",
    "\n",
    "df[\"createdAt\"] = pd.to_datetime(df[\"createdAt\"], errors=\"coerce\")\n",
    "df[\"createdAt\"] = df[\"createdAt\"].dt.strftime(\"%d/%m/%Y %H:%M\")\n",
    "\n",
    "df.to_csv(r\"C:\\Users\\migue\\OneDrive\\Escritorio\\UAB_INTELIGENCIA_ARTIFICIAL\\Tercer_Any\\3B\\Synthesis Project II\\Limpieza\\sin_compañias.csv\", index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8fe097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets de marcas con respuestas: 220\n",
      "                                                     url  replyCount  \\\n",
      "30     https://x.com/cruillabcn/status/18161275900984...           5   \n",
      "76     https://x.com/rodalies/status/1797531252460753276          17   \n",
      "109    https://x.com/cruillabcn/status/18131334874796...           5   \n",
      "118    https://x.com/discosdesalsa/status/18117493745...           9   \n",
      "121    https://x.com/iAvrilSpain_/status/181167939351...           5   \n",
      "...                                                  ...         ...   \n",
      "21552  https://x.com/ClipsterBCN/status/7498774364987...           4   \n",
      "22208  https://x.com/EstrellaDammCat/status/620560910...           4   \n",
      "23252  https://x.com/EstrellaDammCat/status/618668342...          32   \n",
      "23272  https://x.com/EstrellaDammCat/status/618384083...          26   \n",
      "23285  https://x.com/EstrellaDammCat/status/618131553...          68   \n",
      "\n",
      "       author_userName  \n",
      "30          cruillabcn  \n",
      "76            rodalies  \n",
      "109         cruillabcn  \n",
      "118      discosdesalsa  \n",
      "121       iavrilspain_  \n",
      "...                ...  \n",
      "21552      clipsterbcn  \n",
      "22208  estrelladammcat  \n",
      "23252  estrelladammcat  \n",
      "23272  estrelladammcat  \n",
      "23285  estrelladammcat  \n",
      "\n",
      "[220 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df_marcas_con_respuestas = df_marcas[df_marcas[\"replyCount\"] > 3].copy()\n",
    "\n",
    "print(f\"Tweets de marcas con respuestas: {len(df_marcas_con_respuestas)}\")\n",
    "print(df_marcas_con_respuestas[[\"url\", \"replyCount\", \"author_userName\"]])\n",
    "df_marcas_con_respuestas = df_marcas_con_respuestas[[\"url\"]]\n",
    "df_marcas_con_respuestas.to_csv(r\"C:\\Users\\migue\\OneDrive\\Escritorio\\UAB_INTELIGENCIA_ARTIFICIAL\\Tercer_Any\\3B\\Synthesis Project II\\Limpieza\\compañias_respuestas.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ab8ab22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets de marcas con respuestas: 82\n",
      "                                                     url  replyCount  \\\n",
      "63      https://x.com/el_pais/status/1814617509405831661           8   \n",
      "83     https://x.com/PlanetaAmaral/status/18124401785...           5   \n",
      "156    https://x.com/MBComunidad/status/1811177199585...           5   \n",
      "160       https://x.com/YOL76/status/1812490624785887379          25   \n",
      "202    https://x.com/MBComunidad/status/1807880119349...           4   \n",
      "...                                                  ...         ...   \n",
      "20332   https://x.com/Fnac_CAT/status/878178336211480576           7   \n",
      "22391  https://x.com/Candeloide/status/62007071356789...          10   \n",
      "22761  https://x.com/jamiecullum/status/6198361887535...           6   \n",
      "24043  https://x.com/Helenity_/status/487866871677714432           5   \n",
      "24064  https://x.com/MartaSpons/status/48776051240364...           5   \n",
      "\n",
      "      author_userName  \n",
      "63            el_pais  \n",
      "83      planetaamaral  \n",
      "156       mbcomunidad  \n",
      "160             yol76  \n",
      "202       mbcomunidad  \n",
      "...               ...  \n",
      "20332        fnac_cat  \n",
      "22391      candeloide  \n",
      "22761     jamiecullum  \n",
      "24043       helenity_  \n",
      "24064      martaspons  \n",
      "\n",
      "[82 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df_con_respuestas = df[df[\"replyCount\"] > 3].copy()\n",
    "\n",
    "print(f\"Tweets de marcas con respuestas: {len(df_con_respuestas)}\")\n",
    "print(df_con_respuestas[[\"url\", \"replyCount\", \"author_userName\"]])\n",
    "df_con_respuestas = df_con_respuestas[[\"url\"]]\n",
    "df_con_respuestas.to_csv(r\"C:\\Users\\migue\\OneDrive\\Escritorio\\UAB_INTELIGENCIA_ARTIFICIAL\\Tercer_Any\\3B\\Synthesis Project II\\Limpieza\\no_compañias_respuestas.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0f9588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import os\n",
    "comments = pd.read_csv(r\"C:\\Users\\migue\\OneDrive\\Escritorio\\UAB_INTELIGENCIA_ARTIFICIAL\\Tercer_Any\\3B\\Synthesis Project II\\Limpieza\\all_comments_definitivo.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c0d862b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2303 entries, 0 to 2302\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               2303 non-null   int64 \n",
      " 1   Post Text        2281 non-null   object\n",
      " 2   Post Date        2303 non-null   object\n",
      " 3   Post URL         2303 non-null   object\n",
      " 4   Post Language    2303 non-null   object\n",
      " 5   Author Username  2303 non-null   object\n",
      " 6   Author Location  2303 non-null   object\n",
      " 7   Reply To User    2303 non-null   object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 144.1+ KB\n"
     ]
    }
   ],
   "source": [
    "columnas_a_eliminar = [\n",
    "    'Post Likes', 'Post Replies',\"Post Views\" ,'Post Reposts','Post Quotes', \n",
    "    'Post Bookmarks', 'Is Retweet', \"Author Name\",\n",
    "    'Author URL', 'Author Description', 'Author Followers', 'Author Following',\n",
    "    'Author Posts Count', 'Author Join Date', 'Author Is Verified',\n",
    "    'Author Profile Image', 'Author Profile Banner',\n",
    "    'Author Media Count', 'Media URL 1', 'Media URL 2', 'Media URL 3',\n",
    "    'Media URL 4', 'Media Types', 'Has Media', 'Media Count',\n",
    "    'Is Reply', 'Is Quote Tweet', 'Is Translatable',\n",
    "    'Is Editable', 'Edits Remaining'\n",
    "]\n",
    "\n",
    "comments = comments.drop(columns=columnas_a_eliminar, errors=\"ignore\")\n",
    "comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3bc94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\AppData\\Local\\Temp\\ipykernel_6212\\1683321105.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  comments[\"createdAt\"] = pd.to_datetime(comments[\"createdAt\"], errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV limpio guardado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# ========= 0. NORMALIZAR fullText (primera letra mayúscula, resto minúsculas) =========\n",
    "\n",
    "def normalizar_fulltext(texto):\n",
    "    if pd.isna(texto):\n",
    "        return texto\n",
    "    return texto.lower()\n",
    "\n",
    "comments[\"Post Text\"] = comments[\"Post Text\"].apply(normalizar_fulltext)\n",
    "\n",
    "# ========= 2. QUITAR DUPLICADOS POR ID =========\n",
    "\n",
    "comments = comments.drop_duplicates(subset=[\"id\"], keep=\"first\")\n",
    "\n",
    "# ========= 3. LIMPIAR TEXTOS: EMOJIS Y HASHTAGS =========\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    if pd.isna(texto):\n",
    "        return texto\n",
    "    texto = re.sub(r'http\\S+|www.\\S+', '', texto)\n",
    "    texto = re.sub(r'https\\S+|www.\\S+', '', texto)\n",
    "    # Quitar menciones @\n",
    "    #texto = re.sub(r'@\\w+', '', texto)\n",
    "    # Quitar hashtags\n",
    "    texto = re.sub(r'#\\w+', '', texto)\n",
    "    texto = emoji.replace_emoji(texto, replace='')\n",
    "    # Quitar espacios extra\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    return texto\n",
    "\n",
    "# Cambiar nombres de columnas\n",
    "comments.rename(columns={\n",
    "    'Post Text': 'fullText',\n",
    "    'Post Date': 'createdAt',\n",
    "    'Post URL': 'url',\n",
    "    'Post Language': 'lang',\n",
    "    'Author Username': 'author_userName',\n",
    "    'Author Location': 'author_location',\n",
    "    # Agrega más si es necesario\n",
    "}, inplace=True)\n",
    "\n",
    "comments[\"fullText\"] = comments[\"fullText\"].apply(limpiar_texto)\n",
    "comments[\"createdAt\"] = pd.to_datetime(comments[\"createdAt\"], errors=\"coerce\")\n",
    "comments[\"createdAt\"] = comments[\"createdAt\"].dt.strftime(\"%d/%m/%Y %H:%M\")\n",
    "\n",
    "marcas = [\n",
    "    \"cruillabcn\", \"barcelonaglobal\", \"btvnoticies\", \"lavanguardia\", \"diaridebcn\", \n",
    "    \"cacaolat\", \"estrelladamm\", \"rodalies\", \"catalannews\", \"movistar_es\", \n",
    "    \"elpaiscatalunya\", \"radioudeg\", \"324cat\", \"naciodigital\", \"spotify\", \n",
    "    \"on_economia\", \"on_economia_cat\", \"diariregio7\", \"elpeoriodico\", \"msn_es\", \"Cati_Tarragona\", \"beteve\", \n",
    "    \"MBecerraTour\", \"nuvol_com\", \"Canal21Ebre\", \"MPC_management\", \"elnacionalcat_e\",\n",
    "    \"EPSI_UAB\", \"mundodeportivo\", \"cati_tarragona\", \"elnacionalcat\", \"los40\",\n",
    "    \"CatalunyaPlural\", \"ClubRACC\", \"rac105\", \"AlDiaCat\", \"bnmallorca\",\n",
    "    \"subnoise\", \"canal21ebre\", \"ebredigitalcat\", \"aguaitacat\", \"ElReferente\",\n",
    "    \"actualidad_na\", \"ElNacionalWeb\", \"EuropaFMCat\", \"MWCapital\", \"laganzuaweb\",\n",
    "    \"primerafilacat\", \n",
    "]\n",
    "\n",
    "# Convertimos a minúsculas por si acaso\n",
    "comments[\"author_userName\"] = comments[\"author_userName\"].astype(str).str.lower()\n",
    "marcas = [m.lower() for m in marcas]\n",
    "comments[\"Reply To User\"] = comments[\"Reply To User\"].astype(str)\n",
    "\n",
    "# Filtrar filas:\n",
    "# 1. Donde \"Reply to User\" NO sea \"Not Available\"\n",
    "# 2. Y donde ese valor esté en la lista de marcas (insensibilidad a mayúsculas)\n",
    "comments = comments[~(\n",
    "    (comments[\"author_userName\"].str.lower().isin(marcas)) & \n",
    "    (comments[\"Reply To User\"] != \"Not Available\")\n",
    ")]\n",
    "\n",
    "# ========= GUARDAR NUEVO CSV LIMPIO =========\n",
    "\n",
    "comments.to_csv(r\"C:\\Users\\migue\\OneDrive\\Escritorio\\UAB_INTELIGENCIA_ARTIFICIAL\\Tercer_Any\\3B\\Synthesis Project II\\Limpieza\\all_comments_1.0.csv\", encoding=\"utf-8\", index=False)\n",
    "print(\"CSV limpio guardado con éxito.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
